{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnySat Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AnySat is available through PyTorch Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/gastruc/anysat/zipball/main\" to /home/GAstruc/.cache/torch/hub/main.zip\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.hub.load(\n",
    "    \"gastruc/anysat\", \"anysat\", pretrained=True, force_reload=True, flash_attn=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repo installation:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/gastruc/AnySat.git\n",
    "cd AnySat\n",
    "pip install -e AnySat\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hubconf import AnySat\n",
    "\n",
    "model = AnySat.from_pretrained(\n",
    "    \"base\", flash_attn=False\n",
    ")  # Set flash_attn=True if you have flash-attn module installed (url flash attn)\n",
    "# device = \"cuda\" If you want to run on GPU default is cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments Reproduction\n",
    "\n",
    "All experiments are available in the [experiments](https://github.com/gastruc/AnySat/tree/main/experiments) folder.\n",
    "\n",
    "For the reproduction of AnySat envirnoment run:\n",
    "\n",
    "```bash\n",
    "# clone project\n",
    "git clone https://github.com/gastruc/anysat\n",
    "cd anysat\n",
    "\n",
    "# [OPTIONAL] create conda environment\n",
    "conda create -n anysat python=3.9\n",
    "conda activate anysat\n",
    "\n",
    "# install requirements\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Create data folder where you can put your datasets\n",
    "mkdir data\n",
    "# Create logs folder\n",
    "mkdir logs\n",
    "```\n",
    "\n",
    "And Then run the experiment you want:\n",
    "\n",
    "```bash\n",
    "# Run AnySat pretraining on GeoPlex\n",
    "python src/train.py exp=GeoPlex_AnySAT\n",
    "\n",
    "# Run AnySat finetuning on BraDD-S1TS\n",
    "python src/train.py exp=BraDD_AnySAT_FT\n",
    "\n",
    "# Run AnySat linear probing on BraDD-S1TS\n",
    "python src/train.py exp=BraDD_AnySAT_LP\n",
    "```\n",
    "\n",
    "You can modify through hydra all parameters you want. For example to train a Small version of AnySat on GeoPlex datasets, run:\n",
    "\n",
    "```bash\n",
    "python src/train.py exp=GeoPlex_AnySAT model=Any_Small_multi\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on AnySat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Template of data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are gonna use an example from TreeSatAI-TS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "\n",
    "\n",
    "def day_number_in_year(date_arr, place=4):\n",
    "    day_number = []\n",
    "    for date_string in date_arr:\n",
    "        date_object = datetime.strptime(\n",
    "            str(date_string).split(\"_\")[place][:8], \"%Y%m%d\"\n",
    "        )\n",
    "        day_number.append(date_object.timetuple().tm_yday)  # Get the day of the year\n",
    "    return torch.tensor(day_number)\n",
    "\n",
    "\n",
    "with rasterio.open(\".media/Abies_alba_1_1005_WEFL_NLF.tif\") as src:\n",
    "    aerial = torch.FloatTensor(src.read())[:, 2:302, 2:302]\n",
    "\n",
    "with h5py.File(\".media/Abies_alba_1_1005_WEFL_NLF.h5\", \"r\") as file:\n",
    "    s1_dates = day_number_in_year(file[\"sen-1-asc-products\"][:])\n",
    "    s2 = torch.tensor(file[\"sen-2-data\"][:])\n",
    "    s2_dates = day_number_in_year(file[\"sen-2-products\"][:], place=2)\n",
    "\n",
    "s1 = torch.load(\".media/Abies_alba_1_1005_WEFL_NLF.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data. AnySat requires data to be normalized.\n",
    "\n",
    "MEAN_AERIAL = torch.tensor(\n",
    "    [150.89349365234375, 92.7138900756836, 84.85437774658203, 80.70423889160156]\n",
    ").float()\n",
    "STD_AERIAL = torch.tensor(\n",
    "    [36.764923095703125, 27.62498664855957, 22.479450225830078, 26.733688354492188]\n",
    ").float()\n",
    "MEAN_S2 = torch.tensor(\n",
    "    [\n",
    "        4304.32958984375,\n",
    "        4159.2666015625,\n",
    "        4057.776611328125,\n",
    "        4328.951171875,\n",
    "        4571.22119140625,\n",
    "        4644.87109375,\n",
    "        4837.2470703125,\n",
    "        4700.2578125,\n",
    "        2823.264404296875,\n",
    "        2319.97021484375,\n",
    "    ]\n",
    ").float()\n",
    "STD_S2 = torch.tensor(\n",
    "    [\n",
    "        3537.99755859375,\n",
    "        3324.23486328125,\n",
    "        3270.070068359375,\n",
    "        3250.530029296875,\n",
    "        2897.391357421875,\n",
    "        2754.4970703125,\n",
    "        2821.521484375,\n",
    "        2625.952392578125,\n",
    "        1731.56298828125,\n",
    "        1549.3028564453125,\n",
    "    ]\n",
    ").float()\n",
    "MEAN_S1 = torch.tensor(\n",
    "    [3.2893013954162598, -3.682938814163208, 0.6116273403167725]\n",
    ").float()\n",
    "STD_S1 = torch.tensor(\n",
    "    [40.11152267456055, 40.535335540771484, 1.0343183279037476]\n",
    ").float()\n",
    "\n",
    "aerial = (aerial - MEAN_AERIAL[:, None, None]) / STD_AERIAL[:, None, None]\n",
    "s2 = (s2 - MEAN_S2[:, None, None]) / STD_S2[:, None, None]\n",
    "s1 = (s1 - MEAN_S1[:, None, None]) / STD_S1[:, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aerial shape torch.Size([1, 4, 300, 300])\n",
      "s2 shape torch.Size([1, 146, 10, 6, 6])\n",
      "s2_dates shape torch.Size([1, 146])\n",
      "s1 shape torch.Size([1, 60, 3, 6, 6])\n",
      "s1_dates shape torch.Size([1, 60])\n"
     ]
    }
   ],
   "source": [
    "print(\"aerial shape\", aerial.unsqueeze(0).shape)\n",
    "print(\"s2 shape\", s2.unsqueeze(0).shape)\n",
    "print(\"s2_dates shape\", s2_dates.unsqueeze(0).shape)\n",
    "print(\"s1 shape\", s1.unsqueeze(0).shape)\n",
    "print(\"s1_dates shape\", s1_dates.unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get features from an observation of a batch of observations, you need to provide to the model a dictionnary where keys are from the list: \n",
    "| Dataset       | Description                       | Tensor Size                                          | Channels                                  | Resolution |\n",
    "|---------------|-----------------------------------|-----------------------------------------|-------------------------------------------|------------|\n",
    "| aerial        | Single date tensor |Bx4xHxW                                              | RGB, NiR                                  | 0.2m       |\n",
    "| aerial-flair  | Single date tensor |Bx5xHxW                                              | RGB, NiR, Elevation                       | 0.2m       |\n",
    "| spot          | Single date tensor |Bx3xHxW                                              | RGB                                       | 1m         |\n",
    "| naip          | Single date tensor |Bx4xHxW                                               | RGB                                       | 1.25m      |\n",
    "| s2            | Time series tensor |BxTx10xHxW                                          | B2, B3, B4, B5, B6, B7, B8, B8a, B11, B12 | 10m        |\n",
    "| s1-asc        | Time series tensor |BxTx2xHxW                                             | VV, VH                                     | 10m        |\n",
    "| s1            | Time series tensor |BxTx3xHxW                                            | VV, VH, Ratio                             | 10m        |\n",
    "| alos          | Time series tensor |BxTx3xHxW                                            | HH, HV, Ratio                             | 30m        |\n",
    "| l7            | Time series tensor |BxTx6xHxW                                            | B1, B2, B3, B4, B5, B7                    | 30m        |\n",
    "| l8            | Time series tensor |BxTx11xHxW                                           | B8, B1, B2, B3, B4, B5, B6, B7, B9, B10, B11 | 10m        |\n",
    "| modis         | Time series tensor |BxTx7xHxW                                            | B1, B2, B3, B4, B5, B6, B7                | 250m       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"aerial\": aerial.unsqueeze(0),  # 1 batch size, 4 channels, 300x300 pixels\n",
    "    \"s2\": s2.unsqueeze(0),  # 1 batch size, 146 dates, 10 channels, 6x6 pixels\n",
    "    \"s2_dates\": s2_dates.unsqueeze(0),\n",
    "    \"s1\": s1.unsqueeze(0),  # 1 batch size, 60 dates, 10 channels, 6x6 pixels\n",
    "    \"s1_dates\": s1_dates.unsqueeze(0),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that time series requires a `_dates` companion tensor containing the day of the year: 01/01 = 0, 31/12=364."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide on:\n",
    "- **Patch size** (in m, must be a multiple of 10): adjust according to the scale of your tiles and GPU memory. In general, avoid having more than 1024 patches per tile.\n",
    "- **Output type**: Choose between:\n",
    "  - `'tile'`: Single vector per tile\n",
    "  - `'patch'`: A vector per patch\n",
    "  - `'dense'`: A vector per sub-patch. Doubles the size to the vectors\n",
    "  - `'all'`: A vector per patch with class token at first position\n",
    " \n",
    "⚠️ For segmentation tasks, use 'dense' argument!\n",
    " \n",
    "The sub patches are `1x1` pixels for time series and `10x10` pixels for VHR images. If using `output='dense'`, specify the `output_modality`.\n",
    "Scale should divide the spatial cover of all modalities and be a multiple of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "features = model(data, patch_size=10, output=\"tile\")\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "features = model(data, patch_size=10, output=\"patch\")\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "features = model(data, patch_size=20, output=\"patch\")\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "features = model(data, patch_size=60, output=\"patch\")\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 30, 30, 1536])\n"
     ]
    }
   ],
   "source": [
    "features = model(data, patch_size=20, output=\"dense\", output_modality=\"aerial\")\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 6, 1536])\n"
     ]
    }
   ],
   "source": [
    "features = model(data, patch_size=20, output=\"dense\", output_modality=\"s2\")\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
